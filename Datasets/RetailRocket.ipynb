{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of category_tree.csv:\n",
      "   categoryid  parentid\n",
      "0        1016     213.0\n",
      "1         809     169.0\n",
      "2         570       9.0\n",
      "3        1691     885.0\n",
      "4         536    1691.0 \n",
      "\n",
      "First 5 rows of events.csv:\n",
      "       timestamp  visitorid event  itemid  transactionid\n",
      "0  1433221332117     257597  view  355908            NaN\n",
      "1  1433224214164     992329  view  248676            NaN\n",
      "2  1433221999827     111016  view  318965            NaN\n",
      "3  1433221955914     483717  view  253185            NaN\n",
      "4  1433221337106     951259  view  367447            NaN \n",
      "\n",
      "First 5 rows of item_properties_part1.csv:\n",
      "       timestamp  itemid    property                            value\n",
      "0  1435460400000  460429  categoryid                             1338\n",
      "1  1441508400000  206783         888          1116713 960601 n277.200\n",
      "2  1439089200000  395014         400  n552.000 639502 n720.000 424566\n",
      "3  1431226800000   59481         790                       n15360.000\n",
      "4  1431831600000  156781         917                           828513 \n",
      "\n",
      "First 5 rows of item_properties_part2.csv:\n",
      "       timestamp  itemid property            value\n",
      "0  1433041200000  183478      561           769062\n",
      "1  1439694000000  132256      976  n26.400 1135780\n",
      "2  1435460400000  420307      921  1149317 1257525\n",
      "3  1431831600000  403324      917          1204143\n",
      "4  1435460400000  230701      521           769062 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "    \n",
    "datasets = {\"Retail_Rocket\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Retail_Rocket',\n",
    "                              \"file_names\": ['category_tree.csv', 'events.csv', 'item_properties_part1.csv', 'item_properties_part2.csv']},\n",
    "            \"Diginetica\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Diginetica',\n",
    "                              \"file_names\": ['product-categories.csv', 'products.csv', 'train-clicks.csv', 'train-item-views.csv', 'train-purchases.csv', 'train-queries.csv']},\n",
    "            \"LFM-BeyMS\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/LFM-BeyMS/dataset',\n",
    "                              \"file_names\": ['beyms.csv', 'events.csv', 'genre_annotations.csv', 'mainstreaminess.csv', 'ms.csv', 'user_groups.csv']},\n",
    "            \"Beauty\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "                \"file_names\": ['All_Beauty.jsonl', 'meta_All_Beauty.jsonl']},\n",
    "            \"Game\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "                \"file_names\": ['Video_Games.jsonl', 'meta_Video_Games.jsonl']}}\n",
    "dataset = datasets[\"Retail_Rocket\"]\n",
    "directory_path = dataset['path']\n",
    "file_names = dataset[\"file_names\"]\n",
    "for name in file_names:\n",
    "    file_path = os.path.join(directory_path, name)\n",
    "    try:\n",
    "        if 'csv' in file_path:\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif 'json' in file_path:\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "        print(f\"First 5 rows of {name}:\")\n",
    "        print(df.head(), \"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {name} not found in the directory {directory_path}.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {name} is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error parsing {name}. Please check the file for inconsistencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3676813/204098059.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  item_props = df_props_latest.groupby('itemid').apply(aggregate_properties).to_dict()\n",
      "100%|██████████| 417053/417053 [00:05<00:00, 81072.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item sentences JSON이 'retailrocket_item_sentences.json' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 파일 경로 (실제 경로에 맞게 수정)\n",
    "part1_path = os.path.join(directory_path, dataset[\"file_names\"][2])\n",
    "part2_path = os.path.join(directory_path, dataset[\"file_names\"][3])\n",
    "category_tree_path = os.path.join(directory_path, dataset[\"file_names\"][0])\n",
    "\n",
    "# item_properties 파일 2개 로드 및 합치기\n",
    "df_part1 = pd.read_csv(part1_path)\n",
    "df_part2 = pd.read_csv(part2_path)\n",
    "df_props = pd.concat([df_part1, df_part2], ignore_index=True)\n",
    "\n",
    "# 각 (itemid, property)별로 최신 timestamp의 row만 남기기\n",
    "df_props = df_props.sort_values(by=['itemid', 'property', 'timestamp'])\n",
    "df_props_latest = df_props.groupby(['itemid', 'property'], as_index=False).last()\n",
    "\n",
    "# 로드: category_tree.csv (컬럼: categoryid, parentid)\n",
    "df_cat = pd.read_csv(category_tree_path)\n",
    "# parentid가 NaN이면 해당 categoryid를 그대로 사용\n",
    "df_cat['parentid'] = df_cat['parentid'].fillna(df_cat['categoryid'])\n",
    "# 카테고리 매핑 dictionary: categoryid -> parentid\n",
    "cat_dict = df_cat.set_index('categoryid')['parentid'].to_dict()\n",
    "\n",
    "# 각 itemid별로 property 딕셔너리로 정리\n",
    "def aggregate_properties(df):\n",
    "    # df: 같은 itemid에 대한 DataFrame\n",
    "    props = {}\n",
    "    for _, row in df.iterrows():\n",
    "        prop = str(row['property']).strip().lower()\n",
    "        val = str(row['value']).strip()\n",
    "        props[prop] = val\n",
    "    return props\n",
    "\n",
    "item_props = df_props_latest.groupby('itemid').apply(aggregate_properties).to_dict()\n",
    "\n",
    "# 각 itemid에 대해 문장 생성\n",
    "def construct_item_sentence(itemid, props):\n",
    "    # categoryid와 available은 따로 처리\n",
    "    sub_cat = props.get(\"categoryid\", \"\").strip()\n",
    "    availability = props.get(\"available\", \"\").strip()\n",
    "    # availability가 비어있으면 생략, 값이 있다면 \"O\" 또는 \"X\"로 변환 (여기서는 단순히 값이 '1'이면 O, 아니면 X로 가정)\n",
    "    if availability:\n",
    "        availability_str = \"O\" if availability == \"1\" else \"X\"\n",
    "    else:\n",
    "        availability_str = \"\"\n",
    "    \n",
    "    # 상위 카테고리: category_tree에서 sub_cat를 통해 lookup (문자열이 숫자형이라고 가정)\n",
    "    parent_cat = \"\"\n",
    "    if sub_cat:\n",
    "        try:\n",
    "            cat_id = int(sub_cat)\n",
    "            parent_cat = str(cat_dict.get(cat_id, \"\"))\n",
    "        except Exception:\n",
    "            parent_cat = \"\"\n",
    "    \n",
    "    # 나머지 프로퍼티: categoryid와 available 제외하고 정렬하여 \"Property i\" 형태로 나열\n",
    "    other_props = {k: v for k, v in props.items() if k not in [\"categoryid\", \"available\"]}\n",
    "    # 정렬\n",
    "    sorted_props = sorted(other_props.items())\n",
    "    other_strs = []\n",
    "    for i, (k, v) in enumerate(sorted_props, start=1):\n",
    "        other_strs.append(f\"Property {i} ({k}): {v}\")\n",
    "    \n",
    "    # 문장 구성: 값이 없는 항목은 생략\n",
    "    parts = []\n",
    "    parts.append(f\"Parent Category: {parent_cat}\" if parent_cat else \"Parent Category: \")\n",
    "    parts.append(f\"Subcategory: {sub_cat}\" if sub_cat else \"Subcategory: \")\n",
    "    if availability_str:\n",
    "        parts.append(f\"Availability: {availability_str}\")\n",
    "    # 다른 프로퍼티는 있으면 추가\n",
    "    if other_strs:\n",
    "        parts.extend(other_strs)\n",
    "    \n",
    "    sentence = \", \".join(parts) + \".\"\n",
    "    return sentence\n",
    "\n",
    "# 각 itemid별 문장을 dictionary로 생성\n",
    "item_sentences = {}\n",
    "for itemid, props in tqdm(item_props.items()):\n",
    "    sentence = construct_item_sentence(itemid, props)\n",
    "    item_sentences[itemid] = sentence\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open(\"retailrocket_item_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(item_sentences, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Item sentences JSON이 'retailrocket_item_sentences.json' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407580/1407580 [05:50<00:00, 4010.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User interactions JSON이 'retailrocket_user_interactions.json' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 파일 경로 (실제 경로에 맞게 수정)\n",
    "events_path = os.path.join(directory_path, dataset[\"file_names\"][1])\n",
    "df_events = pd.read_csv(events_path)\n",
    "\n",
    "# timestamp를 datetime으로 변환 (Retail Rocket의 timestamp는 밀리초초 단위라고 가정)\n",
    "df_events['datetime'] = pd.to_datetime(df_events['timestamp'], unit='ms')\n",
    "df_events = df_events.sort_values(by=['visitorid', 'datetime'])\n",
    "\n",
    "# 필요한 정보만 추출: (itemid, timestamp, event, transactionid)\n",
    "def extract_event(row):\n",
    "    return {\n",
    "        \"itemid\": row[\"itemid\"],\n",
    "        \"timestamp\": row[\"datetime\"].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"event\": row[\"event\"],\n",
    "        \"transactionid\": row[\"transactionid\"] if pd.notna(row[\"transactionid\"]) else \"\"\n",
    "    }\n",
    "\n",
    "df_events[\"event_info\"] = df_events.apply(extract_event, axis=1)\n",
    "\n",
    "# 사용자(visitorid)별로 그룹화하고, 시간 간격이 3시간(10800초) 이상이면 세션 분리\n",
    "user_sessions = {}\n",
    "three_hours = pd.Timedelta(seconds=10800)\n",
    "\n",
    "for visitor, group in tqdm(df_events.groupby('visitorid')):\n",
    "    group = group.sort_values('datetime')\n",
    "    sessions = []\n",
    "    current_session = []\n",
    "    prev_time = None\n",
    "    for _, row in group.iterrows():\n",
    "        current_time = row['datetime']\n",
    "        if prev_time is not None and (current_time - prev_time) >= three_hours:\n",
    "            if current_session:\n",
    "                sessions.append(current_session)\n",
    "            current_session = []\n",
    "        current_session.append(row[\"event_info\"])\n",
    "        prev_time = current_time\n",
    "    if current_session:\n",
    "        sessions.append(current_session)\n",
    "    user_sessions[visitor] = sessions\n",
    "\n",
    "# JSON 파일로 저장 (각 사용자별 세션을 포함)\n",
    "with open(\"retailrocket_user_interactions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_sessions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"User interactions JSON이 'retailrocket_user_interactions.json' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered interactions saved to 'retailrocket_user_interactions_filtered.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 기존 interaction JSON 파일 로드\n",
    "with open(\"retailrocket_user_interactions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_sessions = json.load(f)\n",
    "\n",
    "filtered_user_sessions = {}\n",
    "\n",
    "for user, sessions in user_sessions.items():\n",
    "    total_interactions = sum(len(session) for session in sessions)\n",
    "    # 사용자별 세션 수가 3개 이상이고, 총 interaction 수가 5회 이상인 사용자만 선택\n",
    "    if len(sessions) >= 2 and total_interactions >= 3:\n",
    "        filtered_user_sessions[user] = sessions\n",
    "\n",
    "# 필터링 결과를 새로운 JSON 파일로 저장\n",
    "with open(\"retailrocket_user_interactions_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_user_sessions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Filtered interactions saved to 'retailrocket_user_interactions_filtered.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407580/1407580 [00:00<00:00, 3024449.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 1407580\n",
      "Average sessions per user: 1.196835703832109\n",
      "Average interactions per session: 1.6360158419414925\n",
      "Variance of interactions per session: 11.471367550718051\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import statistics\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(\"retailrocket_user_interactions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_sessions = json.load(f)\n",
    "\n",
    "# 사용자 수\n",
    "num_users = len(user_sessions)\n",
    "\n",
    "# 사용자별 세션 수와 전체 세션별 interaction 수 목록 계산\n",
    "session_counts = []         # 각 사용자별 세션 수\n",
    "interaction_counts = []     # 각 세션의 interaction 수\n",
    "\n",
    "for user, sessions in tqdm(user_sessions.items()):\n",
    "    session_counts.append(len(sessions))\n",
    "    for session in sessions:\n",
    "        interaction_counts.append(len(session))\n",
    "\n",
    "# 사용자당 세션 수의 평균 계산\n",
    "avg_sessions_per_user = sum(session_counts) / num_users if num_users > 0 else 0\n",
    "\n",
    "# 각 세션당 interaction 수의 평균과 분산 계산\n",
    "if interaction_counts:\n",
    "    avg_interactions = statistics.mean(interaction_counts)\n",
    "    var_interactions = statistics.variance(interaction_counts) if len(interaction_counts) > 1 else 0\n",
    "else:\n",
    "    avg_interactions = 0\n",
    "    var_interactions = 0\n",
    "\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Average sessions per user:\", avg_sessions_per_user)\n",
    "print(\"Average interactions per session:\", avg_interactions)\n",
    "print(\"Variance of interactions per session:\", var_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96057/96057 [00:00<00:00, 1001074.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 96057\n",
      "Average sessions per user: 3.2782618653507813\n",
      "Average interactions per session: 2.679044140997142\n",
      "Variance of interactions per session: 49.42560094597455\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import statistics\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(\"retailrocket_user_interactions_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_sessions = json.load(f)\n",
    "\n",
    "# 사용자 수\n",
    "num_users = len(user_sessions)\n",
    "\n",
    "# 사용자별 세션 수와 전체 세션별 interaction 수 목록 계산\n",
    "session_counts = []         # 각 사용자별 세션 수\n",
    "interaction_counts = []     # 각 세션의 interaction 수\n",
    "\n",
    "for user, sessions in tqdm(user_sessions.items()):\n",
    "    session_counts.append(len(sessions))\n",
    "    for session in sessions:\n",
    "        interaction_counts.append(len(session))\n",
    "\n",
    "# 사용자당 세션 수의 평균 계산\n",
    "avg_sessions_per_user = sum(session_counts) / num_users if num_users > 0 else 0\n",
    "\n",
    "# 각 세션당 interaction 수의 평균과 분산 계산\n",
    "if interaction_counts:\n",
    "    avg_interactions = statistics.mean(interaction_counts)\n",
    "    var_interactions = statistics.variance(interaction_counts) if len(interaction_counts) > 1 else 0\n",
    "else:\n",
    "    avg_interactions = 0\n",
    "    var_interactions = 0\n",
    "\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Average sessions per user:\", avg_sessions_per_user)\n",
    "print(\"Average interactions per session:\", avg_interactions)\n",
    "print(\"Variance of interactions per session:\", var_interactions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
