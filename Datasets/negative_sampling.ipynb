{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged candidate set saved to Globo/precomputed_candidate_set_merged.npz\n",
      "Merged candidate set saved to LFM-BeyMS/precomputed_candidate_set_merged.npz\n",
      "Merged candidate set saved to Retail_Rocket/precomputed_candidate_set_merged.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def merge_npz_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    주어진 폴더 내에 있는 precomputed_candidate_set_*.npz 파일들을 하나로 병합합니다.\n",
    "    최종 결과는 folder_path 내에 precomputed_candidate_set_merged.npz 파일로 저장됩니다.\n",
    "    \"\"\"\n",
    "    # 폴더 내의 npz 파일들을 glob로 검색\n",
    "    npz_files = sorted(glob.glob(os.path.join(folder_path, \"precomputed_candidate_set_*.npz\")))\n",
    "    if len(npz_files) == 0:\n",
    "        print(f\"No npz files found in {folder_path}\")\n",
    "        return\n",
    "\n",
    "    merged_dict = {}\n",
    "    for file in npz_files:\n",
    "        data = np.load(file)\n",
    "        keys = data[\"keys\"]\n",
    "        values = data[\"values\"]\n",
    "        for key, val in zip(keys, values):\n",
    "            merged_dict[int(key)] = val.tolist()  # 후보 목록은 리스트 형태\n",
    "\n",
    "    # 정렬된 키와 대응하는 후보 배열 생성\n",
    "    all_keys = np.array(sorted(merged_dict.keys()), dtype=np.int32)\n",
    "    all_values = np.array([merged_dict[k] for k in all_keys], dtype=np.int32)\n",
    "    \n",
    "    output_file = os.path.join(folder_path, \"precomputed_candidate_set_merged.npz\")\n",
    "    np.savez_compressed(output_file, keys=all_keys, values=all_values)\n",
    "    print(f\"Merged candidate set saved to {output_file}\")\n",
    "\n",
    "# 병합할 폴더 목록 (예: Globo, LFM-BeyMS, Retail_Rocket)\n",
    "folders = [\"Globo\", \"LFM-BeyMS\", \"Retail_Rocket\"]\n",
    "\n",
    "for folder in folders:\n",
    "    if os.path.isdir(folder):\n",
    "        merge_npz_in_folder(folder)\n",
    "    else:\n",
    "        print(f\"Folder {folder} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 기본 라이브러리 import 및 설정\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 후보 후보 개수 및 비율 설정\n",
    "candidate_size = 128   # 전체 후보 개수\n",
    "use_top = 30           # 학습 시 실제 사용할 상위 후보 개수\n",
    "\n",
    "# 카테고리별 가중치 offset (hard 후보는 상위에, easy 후보는 낮게)\n",
    "offsets = {\n",
    "    'hard': 0.2,\n",
    "    'rand': 0.0,\n",
    "    'easy': -0.2\n",
    "}\n",
    "# 노이즈 범위 (랜덤 노이즈 추가)\n",
    "noise_range = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './Globo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: precomputed item 임베딩 불러오기 및 embedding matrix 생성\n",
    "embedding_pickle_path = \"item_embedding.pickle\"  # 실제 파일 경로로 수정\n",
    "with open(folder+embedding_pickle_path, \"rb\") as f:\n",
    "    item_embeddings_dict = pickle.load(f)  # { item_id (str): np.array([...]) }\n",
    "\n",
    "# 모든 아이템의 아이디를 정렬하고, 매핑을 만듭니다.\n",
    "# 패딩 처리를 위해 인덱스 0은 예약(0 벡터)으로 설정\n",
    "item_ids = sorted(item_embeddings_dict.keys(), key=lambda x: int(x))\n",
    "mapping = {int(item_id): idx + 1 for idx, item_id in enumerate(item_ids)}\n",
    "num_items = len(item_ids)\n",
    "\n",
    "base_dim = 384  # 원래 임베딩 차원\n",
    "embedding_list = [np.zeros((base_dim,), dtype=np.float32)]  # index 0: 패딩\n",
    "for item_id in item_ids:\n",
    "    embedding_list.append(item_embeddings_dict[item_id])\n",
    "embedding_matrix = np.stack(embedding_list, axis=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 임베딩 정규화 및 row 단위 similarity 계산에 사용할 정규화된 임베딩 생성\n",
    "# 실제 아이템은 index 1부터 사용 (padding은 index 0)\n",
    "real_embeddings = embedding_matrix[1:]  # shape [num_items, 384]\n",
    "norms = torch.norm(real_embeddings, dim=1, keepdim=True)\n",
    "embedding_norm = real_embeddings / (norms + 1e-8)  # [num_items, 384]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e376ab5bc3405b9a0253af5479b8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precomputing candidate sets:   0%|          | 0/3640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 후보 세트 개수: 3640\n",
      "Sample candidate set for item 1: [63442, 367, 88, 936, 162, 209, 82, 12319, 215, 1322, 16, 11136, 723, 491, 573, 14418, 438, 4, 597, 434, 5991, 34242, 892, 572, 498, 3927, 187, 241, 639, 391, 646, 189, 11, 289, 5927, 2379, 222, 111, 210, 3614, 231, 5, 186, 201401, 795, 93, 11127, 112, 84, 941, 5839, 34382, 54487, 216, 8836, 4591, 363161, 113892, 159765, 12557, 102957, 311869, 84977, 209145, 204854, 247562, 322256, 250736, 273597, 288624, 94829, 5519, 134041, 104448, 220498, 83540, 342709, 66391, 270778, 93147, 133776, 358511, 114063, 19303, 176515, 262223, 289557, 197804, 244392, 108477, 76833, 351009, 126268, 161634, 350623, 158521, 268864, 137074, 187195, 138065, 160947, 178456, 182175, 274692, 21973, 17880, 194935, 183599, 187983, 187926, 186505, 191142, 164302, 166547, 179288, 187949, 278838, 187956, 197723, 194706, 193942, 196567, 165948, 253795, 9788, 186503, 185289, 1]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 각 아이템별 candidate set 미리 계산 (OOM 방지를 위해 row 단위 계산)\n",
    "candidate_set_dict = {}\n",
    "\n",
    "for i in tqdm(range(num_items//100), desc=\"Precomputing candidate sets\"):\n",
    "    # 해당 아이템의 normalized embedding (shape [1, 384])\n",
    "    current_emb = embedding_norm[i:i+1]  # [1, 384]\n",
    "    # cosine similarity: [num_items]\n",
    "    sim_vector = torch.matmul(current_emb, embedding_norm.t()).squeeze(0)\n",
    "    # 자기 자신은 제외: 자기 자신 인덱스 i는 제외\n",
    "    sim_vector[i] = -float('inf')\n",
    "    \n",
    "    # move to CPU, numpy for sorting and further processing\n",
    "    sim_np = sim_vector.cpu().numpy()\n",
    "    \n",
    "    # 정렬: 내림차순 (높은 similarity부터)\n",
    "    sorted_indices = np.argsort(-sim_np)  # indices in [0, num_items-1]\n",
    "    \n",
    "    # Hard negatives: top n_hard 후보 (예: 상위 40%: n_hard = int(candidate_size * 0.4))\n",
    "    n_hard = int(candidate_size * 0.4)\n",
    "    # Easy negatives: 하위 n_easy 후보 (예: 하위 20%)\n",
    "    n_easy = int(candidate_size * 0.2)\n",
    "    n_rand = candidate_size - n_hard - n_easy  # 나머지\n",
    "    \n",
    "    hard_candidates = sorted_indices[:n_hard]\n",
    "    easy_candidates = sorted_indices[-n_easy:]\n",
    "    middle_candidates = sorted_indices[n_hard:-n_easy] if n_easy > 0 else sorted_indices[n_hard:]\n",
    "    if len(middle_candidates) < n_rand:\n",
    "        rand_candidates = middle_candidates\n",
    "    else:\n",
    "        rand_candidates = np.random.choice(middle_candidates, size=n_rand, replace=False)\n",
    "    \n",
    "    # 각 후보에 대해 카테고리 정보를 함께 저장\n",
    "    hard_list = [(idx, sim_np[idx], 'hard') for idx in hard_candidates]\n",
    "    rand_list = [(idx, sim_np[idx], 'rand') for idx in rand_candidates]\n",
    "    easy_list = [(idx, sim_np[idx], 'easy') for idx in easy_candidates]\n",
    "    candidate_tuples = hard_list + rand_list + easy_list\n",
    "    \n",
    "    # 각 후보에 대해 weighted score 계산: score = similarity + offset(category) + noise\n",
    "    scored_candidates = []\n",
    "    for cand in candidate_tuples:\n",
    "        idx_val, sim_val, cat = cand\n",
    "        noise = np.random.uniform(0, noise_range)\n",
    "        score = sim_val + offsets[cat] + noise\n",
    "        scored_candidates.append((idx_val, score))\n",
    "    \n",
    "    # 최종 후보: score 기준 내림차순 정렬 후, candidate_size 개 선택\n",
    "    scored_candidates.sort(key=lambda x: -x[1])\n",
    "    final_indices = [sc[0] for sc in scored_candidates][:candidate_size]\n",
    "    \n",
    "    # 최종 후보 item id는: int(item_ids[index]) \n",
    "    candidate_item_ids = [int(item_ids[idx]) for idx in final_indices]\n",
    "    \n",
    "    # 저장: 현재 아이템의 실제 id는 int(item_ids[i])\n",
    "    candidate_set_dict[int(item_ids[i])] = candidate_item_ids\n",
    "\n",
    "print(\"전체 후보 세트 개수:\", len(candidate_set_dict))\n",
    "sample_item = list(candidate_set_dict.keys())[0]\n",
    "print(\"Sample candidate set for item {}:\".format(sample_item), candidate_set_dict[sample_item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate set saved to ./Globo/precomputed_candidate_set.pickle\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: 미리 계산한 후보 세트 저장 (pickle)\n",
    "output_candidate_pickle = \"precomputed_candidate_set.pickle\"\n",
    "with open(folder+output_candidate_pickle, \"wb\") as f:\n",
    "    pickle.dump(candidate_set_dict, f)\n",
    "print(\"Candidate set saved to\", folder+output_candidate_pickle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
