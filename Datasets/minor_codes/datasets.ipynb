{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of beyms.csv:\n",
      "   user_id\n",
      "0  1049656\n",
      "1  1055118\n",
      "2  1056935\n",
      "3  1070023\n",
      "4  1072752 \n",
      "\n",
      "First 5 rows of events.csv:\n",
      "    user_id  artist_id  album_id  track_id   timestamp\n",
      "0  31435741         21        31        53  1370977938\n",
      "1  31435741         21        31        53  1370977728\n",
      "2  31435741         21        31        53  1370977518\n",
      "3  31435741         21        31        53  1370977308\n",
      "4  31435741         21        31        53  1370977098 \n",
      "\n",
      "First 5 rows of genre_annotations.csv:\n",
      "   Unnamed: 0  track_id                                             genres\n",
      "0           1      4868  ['soul', 'pop', 'singersongwriter', 'blues', '...\n",
      "1           2      2900  ['electronic', 'indiepop', 'shoegaze', 'dreamp...\n",
      "2           5    572665  ['soul', 'pop', 'singersongwriter', 'blues', '...\n",
      "3           6      2897  ['indierock', 'electronic', 'indiepop', 'postp...\n",
      "4           7     15100  ['folk', 'indiefolk', 'banjo', 'folkrock', 'bl... \n",
      "\n",
      "First 5 rows of mainstreaminess.csv:\n",
      "   Unnamed: 0   user_id  M_global_R_APC\n",
      "0           0   6823936        0.232189\n",
      "1           5  23937043        0.288424\n",
      "2           9   8259615        0.098305\n",
      "3          15   4030517        0.241614\n",
      "4          16   4933688       -0.026237 \n",
      "\n",
      "First 5 rows of ms.csv:\n",
      "    user_id\n",
      "0  50900118\n",
      "1  43009758\n",
      "2  13959381\n",
      "3  12531653\n",
      "4  40351516 \n",
      "\n",
      "First 5 rows of user_groups.csv:\n",
      "   user_id  user group\n",
      "0  1049656           2\n",
      "1  1055118           1\n",
      "2  1056935           1\n",
      "3  1070023           1\n",
      "4  1072752           0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "    \n",
    "datasets = {\"Retail_Rocket\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Retail_Rocket',\n",
    "                              \"file_names\": ['category_tree.csv', 'events.csv', 'item_properties_part1.csv', 'item_properties_part2.csv']},\n",
    "            \"Diginetica\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Diginetica',\n",
    "                              \"file_names\": ['product-categories.csv', 'products.csv', 'train-clicks.csv', 'train-item-views.csv', 'train-purchases.csv', 'train-queries.csv']},\n",
    "            \"LFM-BeyMS\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/LFM-BeyMS/dataset',\n",
    "                              \"file_names\": ['beyms.csv', 'events.csv', 'genre_annotations.csv', 'mainstreaminess.csv', 'ms.csv', 'user_groups.csv']},\n",
    "            \"Beauty\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "                \"file_names\": ['All_Beauty.jsonl', 'meta_All_Beauty.jsonl']},\n",
    "            \"Game\": {\"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "                \"file_names\": ['Video_Games.jsonl', 'meta_Video_Games.jsonl']}\n",
    "            }\n",
    "dataset = datasets[\"LFM-BeyMS\"]\n",
    "directory_path = dataset['path']\n",
    "file_names = dataset[\"file_names\"]\n",
    "for name in file_names:\n",
    "    file_path = os.path.join(directory_path, name)\n",
    "    try:\n",
    "        if 'csv' in file_path:\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif 'json' in file_path:\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "        print(f\"First 5 rows of {name}:\")\n",
    "        print(df.head(), \"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {name} not found in the directory {directory_path}.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {name} is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error parsing {name}. Please check the file for inconsistencies.\")\n",
    "\n",
    "# The datasets list now contains DataFrames for each CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of All_Beauty.jsonl:\n",
      "   rating                                      title  \\\n",
      "0       5  Such a lovely scent but not overpowering.   \n",
      "1       4     Works great but smells a little weird.   \n",
      "2       5                                       Yes!   \n",
      "3       1                          Synthetic feeling   \n",
      "4       5                                         A+   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0  This spray is really nice. It smells really go...     []  B00YQ6X8EO   \n",
      "1  This product does what I need it to do, I just...     []  B081TJ8YS3   \n",
      "2                          Smells good, feels great!     []  B07PNNCSP9   \n",
      "3                                     Felt synthetic     []  B09JS339BZ   \n",
      "4                                            Love it     []  B08BZ63GMJ   \n",
      "\n",
      "  parent_asin                       user_id               timestamp  \\\n",
      "0  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-05 14:08:48.923   \n",
      "1  B081TJ8YS3  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-04 18:10:55.070   \n",
      "2  B097R46CSY  AE74DYR3QUGVPZJ3P7RFWBGIX7XQ 2020-05-16 21:41:06.052   \n",
      "3  B09JS339BZ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2022-01-28 18:13:50.220   \n",
      "4  B08BZ63GMJ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-30 10:02:43.534   \n",
      "\n",
      "   helpful_vote  verified_purchase  \n",
      "0             0               True  \n",
      "1             1               True  \n",
      "2             2               True  \n",
      "3             0               True  \n",
      "4             0               True   \n",
      "\n",
      "First 5 rows of meta_All_Beauty.jsonl:\n",
      "  main_category                                              title  \\\n",
      "0    All Beauty  Howard LC0008 Leather Conditioner, 8-Ounce (4-...   \n",
      "1    All Beauty  Yes to Tomatoes Detoxifying Charcoal Cleanser ...   \n",
      "2    All Beauty   Eye Patch Black Adult with Tie Band (6 Per Pack)   \n",
      "3    All Beauty  Tattoo Eyebrow Stickers, Waterproof Eyebrow, 4...   \n",
      "4    All Beauty  Precision Plunger Bars for Cartridge Grips – 9...   \n",
      "\n",
      "   average_rating  rating_number  \\\n",
      "0             4.8             10   \n",
      "1             4.5              3   \n",
      "2             4.4             26   \n",
      "3             3.1            102   \n",
      "4             4.3              7   \n",
      "\n",
      "                                            features  \\\n",
      "0                                                 []   \n",
      "1                                                 []   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4  [Material: 304 Stainless Steel; Brass tip, Len...   \n",
      "\n",
      "                                         description  price  \\\n",
      "0                                                 []    NaN   \n",
      "1                                                 []    NaN   \n",
      "2                                                 []    NaN   \n",
      "3                                                 []    NaN   \n",
      "4  [The Precision Plunger Bars are designed to wo...    NaN   \n",
      "\n",
      "                                              images videos  \\\n",
      "0  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
      "1  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
      "2  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
      "3  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
      "4  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
      "\n",
      "                    store categories  \\\n",
      "0         Howard Products         []   \n",
      "1                  Yes To         []   \n",
      "2  Levine Health Products         []   \n",
      "3                Cherioll         []   \n",
      "4               Precision         []   \n",
      "\n",
      "                                             details parent_asin  \\\n",
      "0  {'Package Dimensions': '7.1 x 5.5 x 3 inches; ...  B01CUPMQZE   \n",
      "1  {'Item Form': 'Powder', 'Skin Type': 'Acne Pro...  B076WQZGPM   \n",
      "2         {'Manufacturer': 'Levine Health Products'}  B000B658RI   \n",
      "3  {'Brand': 'Cherioll', 'Item Form': 'Powder', '...  B088FKY3VD   \n",
      "4                            {'UPC': '644287689178'}  B07NGFDN6G   \n",
      "\n",
      "   bought_together  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Amazon Reviews 데이터셋 (jsonl 파일들)\n",
    "datasets = {\n",
    "    \"Beauty\": {\n",
    "        \"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "        \"file_names\": ['All_Beauty.jsonl', 'meta_All_Beauty.jsonl']\n",
    "    },\n",
    "    \"Game\": {\n",
    "        \"path\": '/home/jy1559/Mar2025_Module/Datasets/Amazon',\n",
    "        \"file_names\": ['Video_Games.jsonl', 'meta_Video_Games.jsonl']\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = datasets[\"Beauty\"]\n",
    "directory_path = dataset['path']\n",
    "file_names = dataset[\"file_names\"]\n",
    "\n",
    "for name in file_names:\n",
    "    file_path = os.path.join(directory_path, name)\n",
    "    try:\n",
    "        # JSON Lines 파일을 읽어서 DataFrame으로 변환 (lines=True 옵션 사용)\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        print(f\"First 5 rows of {name}:\")\n",
    "        print(df.head(), \"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {name} not found in the directory {directory_path}.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {name} is empty.\")\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing {name}. Please check the file for inconsistencies.\")\n",
    "\n",
    "# 이제 각 JSONL 파일이 DataFrame으로 로드됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 94237913 bytes (-60266219 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/retailrocket/ecommerce-dataset?dataset_version_number=2&file_name=events.csv (94237913/33971694) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89.9MB [00:00, 357kB/s]\n"
     ]
    },
    {
     "ename": "DataCorruptionError",
     "evalue": "The X-Goog-Hash header indicated a MD5 checksum of:\n\n  iyXIKbW0H+pLWsmI0V8QJw==\n\nbut the actual MD5 checksum of the downloaded contents was:\n\n  FYwLQSf5dSapKVp8OzJFxw==\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataCorruptionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleDatasetAdapter\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKaggleDatasetAdapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPANDAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretailrocket/ecommerce-dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevents.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/datasets.py:115\u001b[0m, in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter\u001b[38;5;241m.\u001b[39mPANDAS:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datasets\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pandas_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_query\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     not_implemented_error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not yet implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/pandas_datasets.py:89\u001b[0m, in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     86\u001b[0m read_function \u001b[38;5;241m=\u001b[39m _validate_read_function(file_extension, sql_query)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Now that everything has been validated, we can start downloading and processing\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     result \u001b[38;5;241m=\u001b[39m read_function(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;241m*\u001b[39m_build_args(read_function, filepath, sql_query),\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_build_kwargs(file_extension, pandas_kwargs),\n\u001b[1;32m     94\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/datasets.py:35\u001b[0m, in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     33\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[1;32m     34\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 35\u001b[0m path, _ \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/registry.py:28\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/resolver.py:29\u001b[0m, in \u001b[0;36mResolver.__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     path, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     register_datasource_access(handle, version)\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/http_resolver.py:122\u001b[0m, in \u001b[0;36mDatasetHttpResolver._resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Downloading a single file.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(out_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_auto_compressed_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# TODO(b/345800027) Implement parallel download when < 25 files in databundle.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     archive_path \u001b[38;5;241m=\u001b[39m get_cached_archive_path(h)\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/kagglehub/clients.py:223\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[0;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_md5_hash \u001b[38;5;241m!=\u001b[39m expected_md5_hash:\n\u001b[1;32m    222\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(out_file)  \u001b[38;5;66;03m# Delete the corrupted file.\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DataCorruptionError(\n\u001b[1;32m    224\u001b[0m             _CHECKSUM_MISMATCH_MSG_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(expected_md5_hash, actual_md5_hash)\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# For individual file downloads, the downloaded file may be a zip of the file rather\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# than the file name/type that was requested (e.g. my-big-table.csv.zip and not my-big-table.csv).\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# If that's the case, we should auto-extract it so users get what they expect.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m expected_downloded_file_name \u001b[38;5;241m=\u001b[39m urlparse(out_file)\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mDataCorruptionError\u001b[0m: The X-Goog-Hash header indicated a MD5 checksum of:\n\n  iyXIKbW0H+pLWsmI0V8QJw==\n\nbut the actual MD5 checksum of the downloaded contents was:\n\n  FYwLQSf5dSapKVp8OzJFxw==\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files: ['item_properties_part2.csv', 'item_properties_part1.csv', 'category_tree.csv']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
