{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b32ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def preprocess_item_embeddings(embedding_file, output_file, device='cpu'):\n",
    "    \"\"\"\n",
    "    embedding_file: str, 원래 item_embedding_normalized.pickle 파일 경로\n",
    "    output_file: str, 출력 파일 (예: item_embedding_normalized_revised.pickle)\n",
    "    device: str, device (주로 CPU로 처리 후 저장)\n",
    "    \n",
    "    pickle 파일은 { item_id: embedding (numpy array) } 형태라고 가정.\n",
    "    \n",
    "    전처리:\n",
    "      - 모든 item id를 int로 변환 후, 유니크하게 정렬\n",
    "      - 0번 인덱스를 padding으로 사용할 것이므로, mapping: {원본_id: new_index} 를 새롭게 만듦, new_index는 1부터 시작\n",
    "      - 각 embedding을 torch.tensor로 변환하고, index 0에는 0벡터 삽입\n",
    "      - 최종적으로, { 'embedding_tensor': tensor, 'id2index': mapping } 을 저장\n",
    "    \"\"\"\n",
    "    # pickle 파일 로드\n",
    "    with open(embedding_file, 'rb') as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    \n",
    "    # 모든 item id 추출 (key가 문자열일 수 있으므로 int로 변환)\n",
    "    raw_ids = [int(key) for key in embedding_dict.keys()]\n",
    "    sorted_ids = sorted(set(raw_ids))\n",
    "\n",
    "    # mapping: 0번을 padding 용으로 사용하므로 new_index는 1부터 시작\n",
    "    id2index = {orig_id: idx+1 for idx, orig_id in enumerate(sorted_ids)}\n",
    "    num_items = len(sorted_ids)\n",
    "    print(f\"Number of unique items: {num_items}, Largest item id: {max(sorted_ids)}, Smallest item id: {min(sorted_ids)}\")\n",
    "\n",
    "    # embedding 차원 확인 (모든 embedding은 동일하다고 가정)\n",
    "    sample_embedding = next(iter(embedding_dict.values()))\n",
    "    embedding_dim = sample_embedding.shape[0] if isinstance(sample_embedding, np.ndarray) else len(sample_embedding)\n",
    "    \n",
    "    # index 0: padding 벡터 (0으로 채움)\n",
    "    embedding_list = [torch.zeros(embedding_dim, dtype=torch.float32)]\n",
    "    \n",
    "    # 정렬된 id 순으로 embedding을 torch.tensor화\n",
    "    for orig_id in sorted_ids:\n",
    "        emb_np = embedding_dict[str(orig_id)] if str(orig_id) in embedding_dict else embedding_dict[orig_id]\n",
    "        emb_tensor = torch.tensor(emb_np, dtype=torch.float32)\n",
    "        embedding_list.append(emb_tensor)\n",
    "    \n",
    "    # stacking하여 tensor로 만들기: shape [num_items+1, embedding_dim]\n",
    "    embedding_tensor = torch.stack(embedding_list, dim=0)\n",
    "    \n",
    "    # 결과 저장: CPU 상의 tensor로 저장\n",
    "    output = {\n",
    "        'embedding_tensor': embedding_tensor,\n",
    "        'id2index': id2index\n",
    "    }\n",
    "    #with open(output_file, 'wb') as f:\n",
    "        #pickle.dump(output, f)\n",
    "    \n",
    "    print(f\"Preprocessed item embeddings saved to {output_file}\")\n",
    "    return output\n",
    "\n",
    "def preprocess_candidate_sets_revised(candidate_file, output_file, id2index):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      candidate_file: str, 원본 candidate_sets.npz 파일 경로.\n",
    "      output_file: str, 저장할 _revised candidate file 경로 (예: candidate_sets_revised.npz)\n",
    "      id2index: dict, 원본 item id -> new index (이미 전처리 단계에서 생성된 mapping)\n",
    "    \n",
    "    전처리:\n",
    "      - npz 파일에서 candidate_keys와 candidate_values를 로드합니다.\n",
    "      - 각 후보 리스트를 재매핑하여, 원본 candidate 개수를 그대로 사용합니다.\n",
    "      - 만약 후보 정보가 부족하면 자기 자신으로 채움.\n",
    "      \n",
    "    Returns:\n",
    "      candidate_tensor: [N+1, candidate_num] long tensor, 여기서 N+1은 전체 item 수 (패딩 포함)\n",
    "                        candidate_num은 원본 파일에 들어있는 후보 개수.\n",
    "    \"\"\"\n",
    "    data = np.load(candidate_file)\n",
    "    candidate_keys = data[\"keys\"]  # 원본 candidate key: 보통 item id (문자열 또는 int)\n",
    "    candidate_values = data[\"values\"]  # 각 원소는 후보 리스트 (예: list of int)\n",
    "\n",
    "    # 임시 dict 생성: 원본 id -> candidate list\n",
    "    temp_candidate_dict = {}\n",
    "    for i, k in enumerate(candidate_keys):\n",
    "        orig_id = int(k)\n",
    "        # 여기서는 후보 개수를 변경하지 않고 그대로 사용합니다.\n",
    "        cand_list = candidate_values[i].tolist()\n",
    "        # 만약 부족하다면, 자기 자신으로 채움\n",
    "        # (이 부분은 상황에 따라 조정 가능)\n",
    "        temp_candidate_dict[orig_id] = cand_list\n",
    "\n",
    "    # 전체 아이템 수: id2index 매핑에 기반 (padding index 0 포함: 전체 개수 = len(id2index)+1)\n",
    "    N = len(id2index) + 1\n",
    "    # 후보 수: 원본 candidate list 중 하나의 길이를 사용 (모두 동일하다고 가정)\n",
    "    # 만약 item마다 다를 경우, 최대 길이를 사용할 수 있음.\n",
    "    candidate_num = max(len(lst) for lst in candidate_values)\n",
    "    \n",
    "    # candidate_tensor 초기화: [N, candidate_num]\n",
    "    candidate_tensor = torch.empty((N, candidate_num), dtype=torch.long)\n",
    "    # index 0은 padding, 보통 자기 자신 혹은 0 값으로 처리 - 여기서는 0으로 채움.\n",
    "    candidate_tensor[0] = torch.zeros(candidate_num, dtype=torch.long)\n",
    "    \n",
    "    for orig_id, new_idx in id2index.items():\n",
    "        if orig_id in temp_candidate_dict:\n",
    "            orig_candidates = temp_candidate_dict[orig_id]  # list of original candidate ids\n",
    "            # 재매핑: 각 candidate id를 id2index를 통해 new index로 변환\n",
    "            new_candidates = []\n",
    "            for cand in orig_candidates:\n",
    "                new_candidates.append(id2index.get(cand, new_idx))  # 매핑이 없으면 자기 자신\n",
    "            # 만약 후보 수가 candidate_num보다 작으면, 부족한 부분을 자기 자신(new_idx)으로 채움\n",
    "            if len(new_candidates) < candidate_num:\n",
    "                new_candidates += [new_idx] * (candidate_num - len(new_candidates))\n",
    "            else:\n",
    "                new_candidates = new_candidates[:candidate_num]\n",
    "            candidate_tensor[new_idx] = torch.tensor(new_candidates, dtype=torch.long)\n",
    "        else:\n",
    "            # 후보 정보가 없는 경우, 자기 자신으로 채움\n",
    "            candidate_tensor[new_idx] = torch.full((candidate_num,), new_idx, dtype=torch.long)\n",
    "    \n",
    "    # 저장: np.savez 사용. tensor는 numpy array로 변환.\n",
    "    #np.savez(output_file, candidate_tensor=candidate_tensor.cpu().numpy())\n",
    "    print(f\"Preprocessed candidate sets saved to {output_file}\")\n",
    "    return candidate_tensor\n",
    "\n",
    "# 예시 인자들 (실제 파일 경로와 candidate_size를 적절히 수정)\n",
    "folder = \"Retail_Rocket/\"\n",
    "item_embedding_file = folder + \"item_embedding_normalized.pickle\"\n",
    "candidate_file = folder + \"candidate_sets.npz\"\n",
    "revised_embedding_file = folder + \"item_embedding_normalized_revised.pickle\"\n",
    "revised_candidate_file = folder + \"candidate_sets_revised.npz\"\n",
    "candidate_size = 128   # 예시 값\n",
    "\n",
    "device = 'cpu'  # 전처리 단계는 CPU에서 진행한 후, 나중에 학습 시 device에 올리면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8369068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 417053, Largest item id: 466866, Smallest item id: 0\n",
      "Preprocessed item embeddings saved to Retail_Rocket/item_embedding_normalized_revised.pickle\n"
     ]
    }
   ],
   "source": [
    "# 1. item embeddings 전처리\n",
    "embedding_output = preprocess_item_embeddings(item_embedding_file, revised_embedding_file, device)\n",
    "id2index = embedding_output['id2index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be20c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed candidate sets saved to Retail_Rocket/candidate_sets_revised.npz\n"
     ]
    }
   ],
   "source": [
    "# 2. candidate sets 전처리\n",
    "candidate_tensor = preprocess_candidate_sets_revised(candidate_file, revised_candidate_file, id2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73938e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 96057\n",
      "14688 users have trash items.\n",
      "Preprocessed interactions saved to Retail_Rocket/interactions_revised.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retail_Rocket/interactions_revised.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_interactions(interaction_file, output_file, id2index):\n",
    "    \"\"\"\n",
    "    interaction_file: str, 원래 interaction.json 파일 경로.\n",
    "    output_file: str, 전처리된 interaction 파일 경로 (예: interactions_revised.json)\n",
    "    id2index: dict, 원래 item id -> new index mapping\n",
    "    \n",
    "    각 interaction 내의 item id를 id2index를 사용해 변환합니다.\n",
    "    만약 변환할 수 없는 id가 있으면, -1 또는 0 (패딩)으로 처리합니다.\n",
    "    \"\"\"\n",
    "    with open(interaction_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # data 형식이 {\"data\": {user_id: sessions, ...}, \"index\": ... } 등이라고 가정\n",
    "    trash_item_user = set()\n",
    "    total_interaction = 0\n",
    "    total_trash_interaction = 0\n",
    "    for user_id, sessions in data[\"data\"].items():\n",
    "        for i, session in enumerate(sessions):\n",
    "            for j, interaction in enumerate(session):\n",
    "                # interaction은 [item_id, timestamp, add_info]\n",
    "                orig_id = interaction[0]\n",
    "                # 만약 orig_id가 -1 (패딩)이면 그대로 유지, 아니면 변환\n",
    "                if orig_id == -1:\n",
    "                    new_id = -1\n",
    "                else:\n",
    "                    # int 변환 후 mapping 적용. 만약 mapping에 없으면 0 (또는 -1) 처리\n",
    "                    total_interaction += 1\n",
    "                    try:\n",
    "                        new_id = id2index[int(orig_id)]\n",
    "                    except Exception as e:\n",
    "                        trash_item_user.add(user_id)\n",
    "                        new_id = -1\n",
    "                        total_trash_interaction += 1\n",
    "                data[\"data\"][user_id][i][j][0] = new_id\n",
    "    print(f\"Number of unique users: {len(data['data'])}\")\n",
    "    print(len(trash_item_user), \"users have trash items.\")\n",
    "    \"\"\"with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\"\"\"\n",
    "    print(f\"Preprocessed interactions saved to {output_file}\")\n",
    "    print(f\"Total interactions: {total_interaction}, Total trash interactions: {total_trash_interaction}\")\n",
    "    return output_file\n",
    "\n",
    "# 사용 예시:\n",
    "# 이미 preprocess_item_embeddings를 통해 id2index를 생성했다면,\n",
    "# 예: embedding_output = preprocess_item_embeddings(embedding_file, revised_embedding_file)\n",
    "#      id2index = embedding_output['id2index']\n",
    "interaction_file = folder + \"interactions.json\"\n",
    "output_file = folder + \"interactions_revised.json\"\n",
    "preprocess_interactions(interaction_file, output_file, id2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a4c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
